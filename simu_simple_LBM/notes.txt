Debugage:

1) compil : ok
2) reduire la complexité du pb
    mpiexec -np 1 ./lpm:
    address not mapped


    mpiexec -np 1 xterm -e gdb ./lbm :
    SISEGV 
    #0  0x00005555555565d7 in setup_init_state_global_poiseuille_profile (mesh=0x7fffffffd550, mesh_type=0x7fffffffd580, mesh_comm=0x7fffffffd590)
    at lbm_init.c:85
    #1  0x00005555555568b8 in setup_init_state (mesh=0x7fffffffd550, mesh_type=0x7fffffffd580, mesh_comm=0x7fffffffd590) at lbm_init.c:155
    #2  0x00005555555554c4 in main (argc=1, argv=0x7fffffffd7d8) at main.c:159

    p Mesh_get_cell(mesh, i, j)
	$2 = (double *) 0x0

	p mesh->cells
	$3 = 0x0
	=>addr non alloué

	le code est mis en commentaire dans lbm_struct.c --'

	les resultats ont l'air bons
3) augmenter le nombre de taches
	mpiexec -np 2 ./lpm:
	blocage a la derniere itérations

	mpiexec -np 1 xterm -e gdb ./lbm   puis arret forcé pour voir ou ils en sont:
		rank 0: dans une barriere dans close_file ou seul le processus de rank 0 accede
		rank 1: dans le finalize

		on enleve la barriere de la fonction et on la met dans le main pour que tout le monde passe dedans


	mpiexec -np 3 ./lpm:
		fonctionne mais pb avec le checksum
	mpiexec -np 4 ./lpm:
		ok
	On rajoute la verification width % nb_x = 0

4) optimisation
	la plupart des barrieres sont inutile

	(lbm_comm.c)les communication des mailles fantomes sont atroces, on envoie les données 1 par 1, encore trop de barrieres
	on regroupe ensembles les communications plutot que de faire des for envoyant une donnée a la fois
	on met un place un systeme ou les rang impair envoi au rang pair puis l'inverse plutot que d avoir un systeme de communication dont le temps depend du nombre de processus
	sauf pour les diagonales ou j ai la flemme de reflechir et ou on passe en comm asynchrone

	y a deux fois l envoi du coin en bas a gauche --' + 2* la phase droite a gauche --'

	//pas fait
	A l'initialisation , on decoupera le travail de facon a ce que le nombre de maille fantome non contigues en mémoire soit inferieur a celui du nombre de maille fantomes contigues


	apres un petit ajout de profiling maison, on se rend compte que tout le temps est passé dans __FLUSH_INOUT__
	le flush inout est un sleep(1)

	
	40% du temps total est passé dans collision
	//meh
	//on inverse les deux boucle afin que le parcours du tableau soit effectué de facon a un petit peu profiter du chargement par ligne de cache
	 x * (9.0/2.0) => x * 4.5 dans compute_equilibrium_profile()