Debugage:

1) compil : ok
2) reduire la complexité du pb
    mpiexec -np 1 ./lpm:
    address not mapped


    mpiexec -np 1 xterm -e gdb ./lbm :
    SISEGV 
    #0  0x00005555555565d7 in setup_init_state_global_poiseuille_profile (mesh=0x7fffffffd550, mesh_type=0x7fffffffd580, mesh_comm=0x7fffffffd590)
    at lbm_init.c:85
    #1  0x00005555555568b8 in setup_init_state (mesh=0x7fffffffd550, mesh_type=0x7fffffffd580, mesh_comm=0x7fffffffd590) at lbm_init.c:155
    #2  0x00005555555554c4 in main (argc=1, argv=0x7fffffffd7d8) at main.c:159

    p Mesh_get_cell(mesh, i, j)
	$2 = (double *) 0x0

	p mesh->cells
	$3 = 0x0
	=>addr non alloué

	le code est mis en commentaire dans lbm_struct.c --'

	les resultats ont l'air bons
3) augmenter le nombre de taches
	mpiexec -np 2 ./lpm:
	blocage a la derniere itérations

	mpiexec -np 2 xterm -e gdb ./lbm   puis arret forcé pour voir ou ils en sont:
		rank 0: dans une barriere dans close_file ou seul le processus de rank 0 accede
		rank 1: dans le finalize

		on enleve la barriere de la fonction et on la met dans le main pour que tout le monde passe dedans


	mpiexec -np 3 ./lpm:
		fonctionne mais pb avec le checksum
	mpiexec -np 4 ./lpm:
		ok
	On rajoute la verification width % nb_x = 0

4) optimisation
	la plupart des barrieres sont inutile

	(lbm_comm.c)les communication des mailles fantomes sont atroces, on envoie les données 1 par 1, encore trop de barrieres
	on regroupe ensembles les communications plutot que de faire des for envoyant une donnée a la fois
	on met un place un systeme ou les rang impair envoi au rang pair puis l'inverse plutot que d avoir un systeme de communication dont le temps depend du nombre de processus
	sauf pour les diagonales ou j ai la flemme de reflechir et ou on passe en comm asynchrone

	y a deux fois l envoi du coin en bas a gauche --' + 2* la phase droite a gauche --'

	//pas fait
	A l'initialisation , on decoupera le travail de facon a ce que le nombre de maille fantome non contigues en mémoire soit inferieur a celui du nombre de maille fantomes contigues


	apres un petit ajout de profiling maison, on se rend compte que tout le temps est passé dans __FLUSH_INOUT__
	le flush inout est un sleep(1)


	write_interval = 1
		35% du temps total est passé dans collision
		30% dans la propagation
		2% dans les communications
		30% dans l'ecriture dans le fichier (avec chaque frame sauvegardé)

	write_interval = 16
		50% du temps total est passé dans collision
		40% dans la propagation
		8% dans les communications
		1% dans l'ecriture dans le fichier (avec chaque frame sauvegardé)

	x * (9.0/2.0) => x * 4.5 dans compute_equilibrium_profile()

	on rajoute le flag de compilation -O3

	dans compute_outflow_zou_he_const_density
		1.0/2.0 => 0.5
		on peux enlever density car = 1

	afin de limiter les parcours du tableau, on pourrai regrouper les fonction collision et propagation (ainsi que special cells plus tard), il faudra cependant adapter les communications car celle ci se trouvent entre les deux étapes
	
	on obtient donc :
		special_cells           : 2.81446%
		collision               : 88.7194%
		lbm_comm_ghost_exchange : 3.18513%
		propagation             : 0.678999%
		save_frame_all_domain   : 4.60199%
		iter time               : 0.00498734s
		total time              : 9.97468s

	afin d'ameliorer les performance sur un noeud, on implemente openmp
	on fera d'abord une premiere implementation avec omp parallel for autour de la boucle de collision
	on passe ensuite a une version ou la region parallele est ouverte avant la boucle des iteration et se termine après, il sera alors necessaire de rajouter des pragma omp master (ou single) sur les opérations autres que les collisions
	//mouais oupa
	après quelques tests, l'utilisation des hyperthreads a un impact sur les performances, cependant, openmp ne les utilise pas par default sur mon pc de test bien que la variable d'environnement OMP_NUM_THREADS ne soit pas 'set', il faudra lancer le programme avec la ligne suivante : OMP_NUM_THREADS=4 mpirun --map-by ppr:1:core ./lbm

	dans les collision on mettra les boucle de facon a lire de facon contigue en mem

	etant donné que nous devons optimiser un cas  a 2 dimensions et 9 direction (et uniquement ce cas la), il est possible de remplacer les boucle de calcul de densité (par exemple) par leur expression directe. on s'evite ainsi quelques opérations, on perd cependant en lisibilité du code


	etat actuel:
		special_cells           : 1.87674%
		collision               : 77.8566%
		lbm_comm_ghost_exchange : 6.65744%
		propagation             : 1.67012%
		save_frame_all_domain   : 11.9391%
		iter time               : 0.00253s
		total time              : 5.05999s
